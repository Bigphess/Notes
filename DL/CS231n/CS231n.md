# Introduction

# Image Classification pipeline
## challenges 
* 图片是由无数数字块组成的
* 视角的转变，亮度的变化，变形都会产生非常大的变化
	* viewpoint
	* illumination
	* deformation
	* occlusion
	* background clutter
	* intraclass variation

## image classifer
* input:image
* output: class_label

## data-driven approach
* 其他方法不行
	* attempts: 边缘检测，纹理等等（但是太过具体）
* 以数据为导向的方法
	* def train(image, label)
	* def predict(model, test_image)

## KNN
### NN
* 对于每一个测试的data，在数据库里面找到离他最近的图片（选择一共找多少张，这么多张里面投票）
* 定义距离（hyperparameter）
	* 曼哈顿距离 L1: 两张图相减求绝对值，然后把整张照片求和
	* 欧几里得距离 L2: 距离的平方和开方
* 实现
	* training：记住每个图片的内容和label
		* image：N✖D，每行是一张图片（拉成一行），一共N张
		* label：1-d数组，sizeN
	* predict：计算距离找到最小的角标（np.argmin)
* 速度：linearly to size of dataset
* 缺点：
	* 预测的时间太长了（expensive）
	* 但是我们希望训练的时间长但是测试的时间短（CNN）

### KNN
* 找到最近的K个，投票
	* 当K增加的时候，整个图片的边缘变得平滑了
	* K的数量也是一个hyperparameter
* 需要选择的hyper（并不能很好的找到最优解）
	* K
	* 用什么distance
	* 如何选择最好的参数
		* 总不能尝试所有的参数吧2333
		* 不能使用test data，请在训练的时候忘记自己拥有它
		* 把train data fold成不同的部分，把其中的一部分当成测试数据（validation data），然后测试训练的结果寻找hyper
		* 交叉验证（cross-validation），循环当validation fold然后average result

### 但是根本不用呢
* 在test time的performance太差了
* 两个图片之间的距离太不直观了，你根本不知道图片间的距离会怎么变

## linear classification
### parametric approach
* 输入：32x32x3的图片，array of numbers 0,1,...3072
* f(x,W) = Wx + b （**在线性分类的情况下**） （10x1）
	* x: image （3072x1 -> 拉直了）
	* W: parameters，weights （10x3027）
	* b： bias （10x1），不是这个函数的参数，只是用来决定比如猫的数量特别多，偏向猫的bias可能就比较大
* 输出：10个数字，表示每个class的scores

### 注意
* W是把不同分类的classifer拼在了一起（乐高一样），每一行都是一个不同的class的分类器，点乘这个图片上面的像素，加上bias就是这个图片最终的得分
* resize所有的图片到一个大小（目前）
* 实际上每个class的score就是图片里面每个点的加权求和，可以想象成在数每个不同地方的点的颜色。如果把W矩阵还原，还原出来的就是这个class的感觉上的颜色
* 可以想象在一个巨高d的space里面，用线性分类

### hard part
* 都用灰度图会有问题
* 相似的texture（？


# loss function optimization
todo：
* 定义一个loss function来定义这个score的好坏
* 找到一个efficiently way去找到minimize 这个loss

## SVM loss
* 假设如果只有三个种类，一张图片对三个class分别会有不同的score。每张图片都可以计算出一个对应的loss
* SVM loss Li = sum max（0，sj - si + 1）
	* si: 想要计算这个的loss function 的class的评分（也就是label标注的class的评分）
	* sj: 这张图对于所有其他种类（除了i）的评分
	* Li: 最终这张图片的loss
	* 1: 是一个safety margin（也是一个hyper parameter）
	* Li的每一项都在0和差值之间找最大值，然后把每一项的加起来求和
* 如何理解这个式子：既然对于不同class的评分越高就是越可能，那么评分是负数的话就说明不可能，这样就直接用0把这种可能性抹去了。如果其他种类在正的方面评分越高，说明这个种类跑偏了，loss越大

![SVM](images/L3_1.jpg)
