# Introduction

# Image Classification pipeline
## challenges 
* 图片是由无数数字块组成的
* 视角的转变，亮度的变化，变形都会产生非常大的变化
	* viewpoint
	* illumination
	* deformation
	* occlusion
	* background clutter
	* intraclass variation

## image classifer
* input:image
* output: class_label

## data-driven approach
* 其他方法不行
	* attempts: 边缘检测，纹理等等（但是太过具体）
* 以数据为导向的方法
	* def train(image, label)
	* def predict(model, test_image)

## KNN
### NN
* 对于每一个测试的data，在数据库里面找到离他最近的图片（选择一共找多少张，这么多张里面投票）
* 定义距离（hyperparameter）
	* 曼哈顿距离 L1: 两张图相减求绝对值，然后把整张照片求和
	* 欧几里得距离 L2: 距离的平方和开方
* 实现
	* training：记住每个图片的内容和label
		* image：N✖D，每行是一张图片（拉成一行），一共N张
		* label：1-d数组，sizeN
	* predict：计算距离找到最小的角标（np.argmin)
* 速度：linearly to size of dataset
* 缺点：
	* 预测的时间太长了（expensive）
	* 但是我们希望训练的时间长但是测试的时间短（CNN）

### KNN
* 找到最近的K个，投票
	* 当K增加的时候，整个图片的边缘变得平滑了
	* K的数量也是一个hyperparameter
* 需要选择的hyper（并不能很好的找到最优解）
	* K
	* 用什么distance
	* 如何选择最好的参数
		* 总不能尝试所有的参数吧2333
		* 不能使用test data，请在训练的时候忘记自己拥有它
		* 把train data fold成不同的部分，把其中的一部分当成测试数据（validation data），然后测试训练的结果寻找hyper
		* 交叉验证（cross-validation），循环当validation fold然后average result

### 但是根本不用呢
* 在test time的performance太差了
* 两个图片之间的距离太不直观了，你根本不知道图片间的距离会怎么变

## linear classification
### parametric approach
* 输入：32x32x3的图片，array of numbers 0,1,...3072
* f(x,W) = Wx + b （**在线性分类的情况下**） （10x1）
	* x: image （3072x1 -> 拉直了）
	* W: parameters，weights （10x3027）
	* b： bias （10x1），不是这个函数的参数，只是用来决定比如猫的数量特别多，偏向猫的bias可能就比较大
* 输出：10个数字，表示每个class的scores

### 注意
* W是把不同分类的classifer拼在了一起（乐高一样），每一行都是一个不同的class的分类器，点乘这个图片上面的像素，加上bias就是这个图片最终的得分
* resize所有的图片到一个大小（目前）
* 实际上每个class的score就是图片里面每个点的加权求和，可以想象成在数每个不同地方的点的颜色。如果把W矩阵还原，还原出来的就是这个class的感觉上的颜色
* 可以想象在一个巨高d的space里面，用线性分类

### hard part
* 都用灰度图会有问题
* 相似的texture（？


# loss function optimization
todo：
* 定义一个loss function来定义这个score的好坏
* 找到一个efficiently way去找到minimize 这个loss

## SVM loss
### 定义
* 假设如果只有三个种类，一张图片对三个class分别会有不同的score。每张图片都可以计算出一个对应的loss
* SVM loss Li = sum max（0，sj - si + 1）
	* si: 想要计算这个的loss function 的class的评分（也就是label标注的class的评分）
	* sj: 这张图对于所有其他种类（除了i）的评分
	* Li: 最终这张图片的loss
	* 1: 是一个safety margin（也是一个hyper parameter）
	* Li的每一项都在0和差值之间找最大值，然后把每一项的加起来求和
* 如何理解这个式子：既然对于不同class的评分越高就是越可能，那么评分是负数的话就说明不可能，这样就直接用0把这种可能性抹去了。如果其他种类在正的方面评分越高，说明这个种类跑偏了，loss越大

![SVM](images/L3_1.jpg)

###注意点
* 在上面这张图里，因为车的评分已经是最高了，计算出来的loss就是0
* 最后再把所有类型的loss求和，除以种类得到最终的loss
* 用的是求和而不是mean也是取决于自己的决定
* 也有的SVM里面用的是max之后平方，但是不平方的用的更多一点，也是一个hyper parameter
* scale
	* 最小：0
	* 最大：infinite 
![python code](images/L3_2.jpg)

### bug
* 在实际应用里面没有那么好的效果
* W不是唯一的，比如把这个W加倍，如果loss是0的时候是一样的 -> 需要得到唯一的W

### weight regularization（解决上面这个问题）
* 在之前的loss的基础上加上了 \lambda R(W)
	* \lambda是一个hyper parameter，是取决于自己的选择的
	* R是一个regularization函数，这个函数的作用是抵抗之前的loss。因为之前的loss是从训练集上得到的，比较吻合训练集，所以需要一个比较特别的W来和之前的fight，这样的话结果可能会在实际使用的时候更好一些
* 主要分类
	* L2 regularization：W里面的所有项平方然后求和（最常见）
	* L1 regularization：W里面所有项绝对值然后求和 -> 在一些其他地方使用
	* elastic net（L1+L2）：所有项平方乘参数加绝对值求和
	* max norm regularization -> 后面讲
	* dropout
* 理解L2
	* 比如X是[1,1,1,1],两个W分别是[1,0,0,0]和[0.25,0.25,0.25,0.25]
	* 这样乘出来的最终结果都是一样的，都是1。
	* 但是如果加上了L2的regularization之后就发现第二种方法的loss更少一点。因为他用到了更多的维数，在实际应用之中效果更好。

## softmax（用起来更好）（multinomial logistic regression）
### 定义
* scores：unnormalized log probabilities of the class -> 需要把score先exp回来，再normalize（除以所有exp之后的的和）
* 最终，对于正确class的最终处理完的score来说，max这个log或者min（loss function）- log会得到最终最好的结果
![softmax function](images/L3_3.jpg)
实际操作如下
![softmax result](images/L3_4.jpg)




