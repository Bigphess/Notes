# 第二章 永远学不懂的线性代数
## 矩阵的乘法
* 一组基x，坐上了矩阵A，A开始带着x一起变化，x下车之后回到原始坐标系，但是实际的位置变化了
* 矩阵的乘法：车上的一组人B（后面的矩阵，看列向量），被公交车的移动矩阵A（看行向量）分别送到不同的地方

## 范数（norm）
* 将向量映射到非负值的函数：本质是比较距离，比如在一维可以直接比较距离，但是二维不能直接比较，求的是欧几里得距离。所以是在不同范围里评价距离的标准。（向量范数）
* p = 2，欧几里得范数（机器学习）
* Frobenius norm（深度学习）（矩阵范数），矩阵元素绝对值的平方和开方。可以横向对比向量范数的L2

## 特征值分解（只能对方阵）
* 特征值就是运动的速度，特征向量就是运动的方向
* 对于矩阵来说，一个运动的初始值和结果可以观测，但是观测不到运动本身，只要附到一个东西上才能观察到
* 特征值分解：
	* 如果矩阵A可以对角化，可以进行特征值分解，两个特征向量中间夹一个对角矩阵
	* 实际上就是把运动分解开了，对角矩阵是只有拉伸（特征值是拉伸的大小），前后的特征向量只有旋转（拉伸的方向）
* 前百分之几的特征值的和就占了全部的99%以上

## 奇异值分解SVD
* 分解为奇异向量和奇异值（可以对每个实数矩阵，应用更多）
* A = UΣV‘
	* 假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量）
* 这两个分解都可以用于图片压缩，因为只取前面百分之几的特征值就可以还原出来大概图片的样子了

## moore-penrose伪逆 -> 假装一个非方阵的逆矩阵

## trace运算 -> 返回矩阵对角元素的和

## 行列式
* 将方阵映射到实数的函数：矩阵特征值的乘积，绝对值用来衡量矩阵参与乘法之后空间扩大或者缩小了多少
* 比如行列式恒等于1，意味着图形的面积不会改变，大于1就是图形变大，等于0就是在一个维度上消失了（等于0的时候矩阵不可逆，因为缩小了一个维度，没有东西能让他们恢复了），小于0就是改变了基的左右手的法则

# 第三章 其实并不懂的概率论和信息论
## 概率分布
* 随机变量：伴随一个概率分布来指定每个状态的可能性
* 协方差：两个变量的线性相关性强度

## 混合分布
* 通过简单的概率分布来构造新的概率分布
* 潜变量（latent veriable）:不能直接观察到的随机变量
* 高斯混合分布：每个组件都是各自不同参数的高斯分布

## 信息论
* 一个不太可能发生的事情发生了，比非常可能发生的事情发生了可以提供更多的信息（从字面定义定义出来了信息的统计单位）
* 对整个概率分布里面的不确定总量进行量化：香浓熵

## 结构化概率模型 -> 大概是一个概率分布可以用图来表示，然后用概率的链式法则表示整个图的概率分布（工具）

# 第四章 数值计算（在过程中迭代和更新）
## 上溢和下溢（底层开发者应该考虑）
* 毁灭性下溢：好多数不是0的时候才有意义
* 破坏性上溢：大量级的数变成了无穷

## 病态条件 -> 对输入的误差特别敏感，会放大本身的误差

## 基于梯度的优化方法
* 
