# 第二章 永远学不懂的线性代数
## 矩阵的乘法
* 一组基x，坐上了矩阵A，A开始带着x一起变化，x下车之后回到原始坐标系，但是实际的位置变化了
* 矩阵的乘法：车上的一组人B（后面的矩阵，看列向量），被公交车的移动矩阵A（看行向量）分别送到不同的地方

## 范数（norm）
* 将向量映射到非负值的函数：本质是比较距离，比如在一维可以直接比较距离，但是二维不能直接比较，求的是欧几里得距离。所以是在不同范围里评价距离的标准。（向量范数）
* p = 2，欧几里得范数（机器学习）
* Frobenius norm（深度学习）（矩阵范数），矩阵元素绝对值的平方和开方。可以横向对比向量范数的L2

## 特征值分解（只能对方阵）
* 特征值就是运动的速度，特征向量就是运动的方向
* 对于矩阵来说，一个运动的初始值和结果可以观测，但是观测不到运动本身，只要附到一个东西上才能观察到
* 特征值分解：
	* 如果矩阵A可以对角化，可以进行特征值分解，两个特征向量中间夹一个对角矩阵
	* 实际上就是把运动分解开了，对角矩阵是只有拉伸（特征值是拉伸的大小），前后的特征向量只有旋转（拉伸的方向）
* 前百分之几的特征值的和就占了全部的99%以上

## 奇异值分解SVD
* 分解为奇异向量和奇异值（可以对每个实数矩阵，应用更多）
* A = UΣV‘
	* 假设A是一个N * M的矩阵，那么得到的U是一个N * N的方阵（里面的向量是正交的，U里面的向量称为左奇异向量），Σ是一个N * M的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V’(V的转置)是一个N * N的矩阵，里面的向量也是正交的，V里面的向量称为右奇异向量）
* 这两个分解都可以用于图片压缩，因为只取前面百分之几的特征值就可以还原出来大概图片的样子了

## moore-penrose伪逆 -> 假装一个非方阵的逆矩阵

## trace运算 -> 返回矩阵对角元素的和

## 行列式
* 将方阵映射到实数的函数：矩阵特征值的乘积，绝对值用来衡量矩阵参与乘法之后空间扩大或者缩小了多少
* 比如行列式恒等于1，意味着图形的面积不会改变，大于1就是图形变大，等于0就是在一个维度上消失了（等于0的时候矩阵不可逆，因为缩小了一个维度，没有东西能让他们恢复了），小于0就是改变了基的左右手的法则

# 第三章 其实并不懂的概率论和信息论
## 概率分布
* 随机变量：伴随一个概率分布来指定每个状态的可能性
* 协方差：两个变量的线性相关性强度

## 混合分布
* 通过简单的概率分布来构造新的概率分布
* 潜变量（latent veriable）:不能直接观察到的随机变量
* 高斯混合分布：每个组件都是各自不同参数的高斯分布

## 信息论
* 一个不太可能发生的事情发生了，比非常可能发生的事情发生了可以提供更多的信息（从字面定义定义出来了信息的统计单位）
* 对整个概率分布里面的不确定总量进行量化：香浓熵

## 结构化概率模型 -> 大概是一个概率分布可以用图来表示，然后用概率的链式法则表示整个图的概率分布（工具）

# 第四章 数值计算（在过程中迭代和更新）
## 上溢和下溢（底层开发者应该考虑）
* 毁灭性下溢：好多数不是0的时候才有意义
* 破坏性上溢：大量级的数变成了无穷

## 病态条件 -> 对输入的误差特别敏感，会放大本身的误差

## 基于梯度的优化方法

# 机器学习基础（为啥我2Q才能选这个课哦？）
## 学习算法
* 从数据中学习的算法，对任务T和性能度量P，经过经验E的改造后可以在p上有所提升

### 任务
* 分类
* 输入缺失分类
* 回归：对给定输入预测数值（除了返回结果形式不太一样，其他的地方和分类比较像）
* 转录：观测非结构化数据，转录为文本形式（语音识别等）
* 机器翻译（自然语言处理）
* 结构化输出：语法分析，像素级分级，输出描述句子
* 异常检测：标记不正常的个体
* 合成和采样：机器学习生成与训练数据相似的新样本
* 去噪
* 密度估计和概率质量函数估计

### 性能度量
* 准确率（accuracy）：输出正确结果的比例
* 错误率（error rate）：输出错误样本的比率
* 对于不同的任务需要的度量不同

### 经验
* supervised算法：每个样本有对应的标签（回归，分类，结构化输出）
* unsupervised算法：没有对应的标签（除了上面三个其他的）
* 强化学习（reinforcement learning）：算法和环境进行交互，学习系统和训练过程有反馈回路

### 线性回归 linear regression（非常简单的一个算法）
* w是决定每个特征影响的权重，如果权重是正的，预测出来的值也会增加
* b是bias参数，指在没有任何值输入时候的偏移
* 度量的方法之一是计算模型在测试集上的均方差（mean square error）

## 容量，过拟合，欠拟合
* 在以前没有测量过的输入上表示良好叫做：generalization（即训练的时候降低的是训练误差，但是需要的是降低测试误差）
* underfitting：欠拟合，拟合的不够
* overfitting：过拟合，拟合的过了
* 通过改变函数的capacity，可以控制模型是否偏向过拟合或者欠拟合（capacity指拟合各种函数的能力，高了容易过拟合）
* non-parametric模型：拟合的时候参数不是固定的，考虑容量是任意高
* 不会有一个算法比其他的算法都好
* 正则化（regularization）：修改学习算法，使其降低泛化误差（测试误差和训练误差的差值）而不是训练误差

## 超参数，验证集
* hyperparameter（超参数）：开始训练之前设置的参数（比如作为容量） -> **因为有的参数不适合学习，容易产生过拟合，所以必须在初始设置好**
* 用于挑选超参数的数据集叫做 validation set（验证集）
* 交叉验证：如果在一个小数据集上面表现的太好了，说明有问题，统计上面有不确定性

## 估计，偏差和方差
* 点估计：试图为感兴趣的量提供单个最优预测
* 偏差：期待和真实值的差，如果在点接近无穷的时候偏差是0，认为是接近无偏差的（asymptotically unbiased)
* **偏差和方差度量着估计量的两个不同误差来源。偏差度量着偏离真实函数或参
数的误差期望。而方差度量着数据上任意特定采样可能导致的估计期望的偏差。**
	* 当容量增大的时候，因为过拟合，偏差会逐渐变小，但是方差会逐渐增大，使得最终泛化误差会呈现一个U的形状
* 一致性：希望数据增多之后统计量的效果会收敛

## 最大似然估计
* 目的：希望有准则在不同的模型里得到特定的函数作为好的估计。利用已知样本结果信息，反推最大概率导致这个结果出现的模型参数值（模型已定，参数未知） ***估计单一参数值的方法***
* 假设：所有的采样都是独立同分布的
* 当p（x| shita）的时候，有两个输入数据，x表示一个具体的数据，shita表示模型的参数
	* 如果shita是确定的，x是变量 -> 概率函数（probability function），描述对于不同样本点x，出现的概率是多少
	* 如果x是确定的，shita是变量 -> **似然函数**（likehood function），描述对于不同模型参数，x这个样本点出现的概率是多少
* 最大似然估计的核心：想办法让观察样本出现的概率最大（当样本数趋近于无穷的时候，估计出来的参数最接近于真实值）

## 贝叶斯统计
* 考虑所有可能的shita值，数据可以被直接观测到（不随机），真是参数是未知的或者不确定的

## 监督学习算法
### 概率监督学习
* 逻辑回归（logistic regression）：解决二分类（0或1）问题的机器学习方法，用于估计某种概率的可能性。
	* 通过sigmoid函数引入非线性元素，处理0/1分类问题
	* sigmoid函数：在远离0的地方，取值会很快接近0或者1，这个特性解决二分类问题很重要

### SVM
* 非概率二分类

### 其他监督学习算法
* KNN
* 决策树

## 无监督学习算法
* 主成分分析
* k-聚值分类

## 随机梯度下降